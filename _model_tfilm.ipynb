{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from models.model import SEANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/woongjib/Projects/MBSEANet\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woongjib/anaconda3/envs/env2/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Input Shape               Output Shape\n",
       "===============================================================================================\n",
       "MBSEANet                                      [4, 5, 55400]             [4, 27, 55400]\n",
       "├─Conv1d: 1-1                                 [4, 5, 55296]             [4, 32, 55296]\n",
       "│    └─Pad: 2-1                               [4, 5, 55296]             [4, 5, 55302]\n",
       "│    └─Conv1d: 2-2                            [4, 5, 55302]             [4, 32, 55296]\n",
       "│    └─ELU: 2-3                               [4, 32, 55296]            [4, 32, 55296]\n",
       "├─ModuleList: 1-2                             --                        --\n",
       "│    └─EncBlock: 2-4                          [4, 32, 55296]            [4, 64, 27648]\n",
       "│    └─EncBlock: 2-5                          [4, 64, 27648]            [4, 128, 13824]\n",
       "│    └─EncBlock: 2-6                          [4, 128, 13824]           [4, 256, 1728]\n",
       "│    └─EncBlock: 2-7                          [4, 256, 1728]            [4, 512, 216]\n",
       "├─Sequential: 1-3                             [4, 512, 216]             [4, 512, 216]\n",
       "│    └─Conv1d: 2-8                            [4, 512, 216]             [4, 128, 216]\n",
       "│    └─Conv1d: 2-9                            [4, 128, 216]             [4, 512, 216]\n",
       "├─ModuleList: 1-4                             --                        --\n",
       "│    └─DecBlock: 2-10                         [4, 512, 216]             [4, 256, 1728]\n",
       "│    └─DecBlock: 2-11                         [4, 256, 1728]            [4, 128, 13824]\n",
       "│    └─DecBlock: 2-12                         [4, 128, 13824]           [4, 64, 27648]\n",
       "│    └─DecBlock: 2-13                         [4, 64, 27648]            [4, 32, 55296]\n",
       "├─Conv1d: 1-5                                 [4, 32, 55296]            [4, 27, 55296]\n",
       "│    └─Pad: 2-14                              [4, 32, 55296]            [4, 32, 55302]\n",
       "│    └─Conv1d: 2-15                           [4, 32, 55302]            [4, 27, 55296]\n",
       "│    └─ELU: 2-16                              [4, 27, 55296]            [4, 27, 55296]\n",
       "===============================================================================================\n",
       "Total params: 8,881,750\n",
       "Trainable params: 8,881,750\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 14.53\n",
       "===============================================================================================\n",
       "Input size (MB): 4.43\n",
       "Forward/backward pass size (MB): 3736.34\n",
       "Params size (MB): 35.53\n",
       "Estimated Total Size (MB): 3776.30\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Multi band SEANet \"\"\"\n",
    "from torchinfo import summary\n",
    "from models.model import MBSEANet\n",
    "\n",
    "model = MBSEANet(c_out=27, c_in=5, min_dim = 32)\n",
    "wav = torch.rand(4,5,55400)\n",
    "\n",
    "summary(\n",
    "    model, input_data = wav,\n",
    "    col_names=['input_size','output_size'],\n",
    "    depth=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([3, 56, 16, 120])\n",
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Kernel Shape\n",
      "===================================================================================================================\n",
      "ResNet                                   [3, 1, 513, 120]          [3, 56, 16, 120]          --\n",
      "├─CausalConv2d: 1-1                      [3, 1, 513, 120]          [3, 7, 256, 120]          [7, 7]\n",
      "├─ReLU: 1-2                              [3, 7, 256, 120]          [3, 7, 256, 120]          --\n",
      "├─MaxPool2d: 1-3                         [3, 7, 256, 120]          [3, 7, 128, 120]          3\n",
      "├─Sequential: 1-4                        [3, 7, 128, 120]          [3, 7, 128, 120]          --\n",
      "│    └─BasicBlock: 2-1                   [3, 7, 128, 120]          [3, 7, 128, 120]          --\n",
      "│    │    └─CausalConv2d: 3-1            [3, 7, 128, 120]          [3, 7, 128, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-2                    [3, 7, 128, 120]          [3, 7, 128, 120]          --\n",
      "│    │    └─CausalConv2d: 3-3            [3, 7, 128, 120]          [3, 7, 128, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-4                    [3, 7, 128, 120]          [3, 7, 128, 120]          --\n",
      "│    └─BasicBlock: 2-2                   [3, 7, 128, 120]          [3, 7, 128, 120]          --\n",
      "│    │    └─CausalConv2d: 3-5            [3, 7, 128, 120]          [3, 7, 128, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-6                    [3, 7, 128, 120]          [3, 7, 128, 120]          --\n",
      "│    │    └─CausalConv2d: 3-7            [3, 7, 128, 120]          [3, 7, 128, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-8                    [3, 7, 128, 120]          [3, 7, 128, 120]          --\n",
      "├─Sequential: 1-5                        [3, 7, 128, 120]          [3, 14, 64, 120]          --\n",
      "│    └─BasicBlock: 2-3                   [3, 7, 128, 120]          [3, 14, 64, 120]          --\n",
      "│    │    └─CausalConv2d: 3-9            [3, 7, 128, 120]          [3, 14, 64, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-10                   [3, 14, 64, 120]          [3, 14, 64, 120]          --\n",
      "│    │    └─CausalConv2d: 3-11           [3, 14, 64, 120]          [3, 14, 64, 120]          [3, 3]\n",
      "│    │    └─CausalConv2d: 3-12           [3, 7, 128, 120]          [3, 14, 64, 120]          [1, 1]\n",
      "│    │    └─ReLU: 3-13                   [3, 14, 64, 120]          [3, 14, 64, 120]          --\n",
      "│    └─BasicBlock: 2-4                   [3, 14, 64, 120]          [3, 14, 64, 120]          --\n",
      "│    │    └─CausalConv2d: 3-14           [3, 14, 64, 120]          [3, 14, 64, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-15                   [3, 14, 64, 120]          [3, 14, 64, 120]          --\n",
      "│    │    └─CausalConv2d: 3-16           [3, 14, 64, 120]          [3, 14, 64, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-17                   [3, 14, 64, 120]          [3, 14, 64, 120]          --\n",
      "├─Sequential: 1-6                        [3, 14, 64, 120]          [3, 28, 32, 120]          --\n",
      "│    └─BasicBlock: 2-5                   [3, 14, 64, 120]          [3, 28, 32, 120]          --\n",
      "│    │    └─CausalConv2d: 3-18           [3, 14, 64, 120]          [3, 28, 32, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-19                   [3, 28, 32, 120]          [3, 28, 32, 120]          --\n",
      "│    │    └─CausalConv2d: 3-20           [3, 28, 32, 120]          [3, 28, 32, 120]          [3, 3]\n",
      "│    │    └─CausalConv2d: 3-21           [3, 14, 64, 120]          [3, 28, 32, 120]          [1, 1]\n",
      "│    │    └─ReLU: 3-22                   [3, 28, 32, 120]          [3, 28, 32, 120]          --\n",
      "│    └─BasicBlock: 2-6                   [3, 28, 32, 120]          [3, 28, 32, 120]          --\n",
      "│    │    └─CausalConv2d: 3-23           [3, 28, 32, 120]          [3, 28, 32, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-24                   [3, 28, 32, 120]          [3, 28, 32, 120]          --\n",
      "│    │    └─CausalConv2d: 3-25           [3, 28, 32, 120]          [3, 28, 32, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-26                   [3, 28, 32, 120]          [3, 28, 32, 120]          --\n",
      "├─Sequential: 1-7                        [3, 28, 32, 120]          [3, 56, 16, 120]          --\n",
      "│    └─BasicBlock: 2-7                   [3, 28, 32, 120]          [3, 56, 16, 120]          --\n",
      "│    │    └─CausalConv2d: 3-27           [3, 28, 32, 120]          [3, 56, 16, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-28                   [3, 56, 16, 120]          [3, 56, 16, 120]          --\n",
      "│    │    └─CausalConv2d: 3-29           [3, 56, 16, 120]          [3, 56, 16, 120]          [3, 3]\n",
      "│    │    └─CausalConv2d: 3-30           [3, 28, 32, 120]          [3, 56, 16, 120]          [1, 1]\n",
      "│    └─BasicBlock: 2-8                   [3, 56, 16, 120]          [3, 56, 16, 120]          --\n",
      "│    │    └─CausalConv2d: 3-31           [3, 56, 16, 120]          [3, 56, 16, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-32                   [3, 56, 16, 120]          [3, 56, 16, 120]          --\n",
      "│    │    └─CausalConv2d: 3-33           [3, 56, 16, 120]          [3, 56, 16, 120]          [3, 3]\n",
      "│    │    └─ReLU: 3-34                   [3, 56, 16, 120]          [3, 56, 16, 120]          --\n",
      "===================================================================================================================\n",
      "Total params: 134,246\n",
      "Trainable params: 134,246\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 34.83\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.74\n",
      "Forward/backward pass size (MB): 54.19\n",
      "Params size (MB): 0.54\n",
      "Estimated Total Size (MB): 55.47\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\" STFT Feature Encoder \"\"\"\n",
    "from torchinfo import summary\n",
    "from models.feature_encoder import ResNet18\n",
    "\n",
    "stft = torch.rand(3,1,513,120) #[B,C,F,T]\n",
    "model = ResNet18(in_channels=7)\n",
    "print('output shape:', model(stft).shape) #[B,C,F,T]=[B,512,16,120]\n",
    "\n",
    "print(summary(\n",
    "    model, input_data = stft,\n",
    "    col_names=['input_size','output_size','kernel_size', ],\n",
    "    depth=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-26 14:18:14.733415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-26 14:18:15.447383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 32]) after pooling\n",
      "torch.Size([1, 12, 10, 32])\n",
      "torch.Size([1, 12, 1, 32])\n",
      "TFiLM(\n",
      "  (max_pool): MaxPool1d(kernel_size=10, stride=10, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lstm): LSTM(32, 32, batch_first=True)\n",
      ")\n",
      "Input shape: torch.Size([1, 120, 32])\n",
      "Output shape: torch.Size([1, 120, 32])\n",
      "torch.Size([1, 12, 32]) after pooling\n",
      "torch.Size([1, 12, 10, 32])\n",
      "torch.Size([1, 12, 1, 32])\n",
      "============================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Kernel Shape              Param #\n",
      "============================================================================================================================================\n",
      "TFiLM                                    [1, 120, 32]              [1, 120, 32]              --                        --\n",
      "├─MaxPool1d: 1-1                         [1, 32, 120]              [1, 32, 12]               10                        --\n",
      "├─LSTM: 1-2                              [1, 12, 32]               [1, 12, 32]               --                        8,448\n",
      "============================================================================================================================================\n",
      "Total params: 8,448\n",
      "Trainable params: 8,448\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.10\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.05\n",
      "============================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer, LSTM, MaxPooling1D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TFiLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Feature-wise Linear Modulation in PyTorch.\n",
    "    \n",
    "    Input shape: (batch_size, steps, num_features)\n",
    "    Output shape: Same as input\n",
    "    \n",
    "    Num Steps must be multiple of block size\n",
    "    \"\"\"\n",
    "    def __init__(self, block_size, input_dim):\n",
    "        super(TFiLM, self).__init__()\n",
    "        self.block_size = block_size\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=self.block_size)\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=input_dim, num_layers=1, batch_first=True)\n",
    "        \n",
    "        \n",
    "    def make_normalizer(self, x):\n",
    "        \"\"\"\n",
    "        Downsamples input along temporal dimension and generates normalization weights via LSTM.\n",
    "        \"\"\"\n",
    "        batch_size, steps, num_features = x.shape\n",
    "        n_blocks = steps // self.block_size\n",
    "\n",
    "        # MaxPool\n",
    "        # print(x.shape, 'xshape:[B,T,F]')\n",
    "        # print(x.permute(0,2,1).shape)\n",
    "        x_down = self.max_pool(x.permute(0,2,1)).permute(0,2,1) # pooling along t axis, block-wise\n",
    "        print(x_down.shape, 'after pooling') # [B,T',F]\n",
    "        \n",
    "        x_rnn, _ = self.lstm(x_down) # [B,T',F] -> [B,T',F]\n",
    "        return x_rnn\n",
    "\n",
    "    def apply_normalizer(self, x, x_norm):\n",
    "        \"\"\"\n",
    "        Applies normalization weights to respective blocks.\n",
    "        \"\"\"\n",
    "        batch_size, steps, num_features = x.shape\n",
    "        n_blocks = steps // self.block_size\n",
    "\n",
    "        x = x.reshape(batch_size, n_blocks, self.block_size, num_features)\n",
    "        print(x.shape) # [B,T/block,block,F]\n",
    "        x_norm = x_norm.reshape(batch_size, n_blocks, 1, num_features)\n",
    "        print(x_norm.shape) #[B,T/block,1,F]\n",
    "\n",
    "        # 정규화 가중치를 적용\n",
    "        x_out = x * x_norm\n",
    "\n",
    "        # 원래 형태로 reshape\n",
    "        x_out = x_out.reshape(batch_size, steps, num_features) #[B,T,F]\n",
    "        return x_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for TFiLM layer.\n",
    "        \"\"\"\n",
    "        # 입력 텐서 형태 확인\n",
    "        assert x.dim() == 3, \"Input must be 3D: (batch_size, steps, num_features)\"\n",
    "        assert x.size(1) % self.block_size == 0, \"Number of steps must be a multiple of block_size\"\n",
    "\n",
    "        x_norm = self.make_normalizer(x)\n",
    "        x_out = self.apply_normalizer(x, x_norm)\n",
    "        return x_out\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    from torchinfo import summary\n",
    "    batch_size = 1\n",
    "    steps = 120  # time steps (must be divisible by block_size)\n",
    "    num_features = 32\n",
    "    block_size = 10\n",
    "\n",
    "    x = torch.randn(batch_size, steps, num_features) #[b,t,f]=[4,16,8]\n",
    "    tfilm = TFiLM(block_size=block_size, input_dim=num_features)\n",
    "    output = tfilm(x)\n",
    "    \n",
    "    print(tfilm)\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    \n",
    "    print(summary(\n",
    "    tfilm, input_data=x,\n",
    "    col_names=['input_size','output_size','kernel_size', 'num_params'],\n",
    "    depth=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 120, 32])\n",
      "torch.Size([1, 12, 10, 32])\n",
      "torch.Size([1, 10, 32])\n",
      "torch.Size([1, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "steps = 120  # time steps (must be divisible by block_size)\n",
    "num_features = 32\n",
    "block_size = 10\n",
    "\n",
    "x = torch.randn(batch_size, steps, num_features) #[b,t,f]=[4,16,8]\n",
    "print(x.shape)\n",
    "xb = x.reshape(x.size(0), x.size(1)//block_size, block_size, x.size(2))\n",
    "print(xb.shape)\n",
    "\n",
    "maxpool = nn.MaxPool1d(kernel_size=12)\n",
    "y = maxpool(x.permute(0,2,1)).permute(0,2,1)\n",
    "print(y.shape)\n",
    "\n",
    "rnn = nn.LSTM(input_size=num_features, hidden_size=num_features, batch_first=True)\n",
    "out,_ = rnn(y)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x shape: torch.Size([1, 120, 32])\n",
      "F_blk shape (Reshaped into blocks): torch.Size([1, 12, 10, 32])\n",
      "F_pool shape (Pooled): torch.Size([1, 12, 32])\n",
      "Gamma shape: torch.Size([1, 12, 32])\n",
      "Beta shape: torch.Size([1, 12, 32])\n",
      "F_norm shape (Normalized): torch.Size([1, 12, 10, 32])\n",
      "Output F_out shape: torch.Size([1, 120, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input 설정\n",
    "batch_size = 1\n",
    "steps = 120  # time steps (must be divisible by block_size)\n",
    "num_features = 32\n",
    "block_size = 10\n",
    "\n",
    "x = torch.randn(batch_size, steps, num_features)  # [B, T, C]\n",
    "print(\"Input x shape:\", x.shape)\n",
    "\n",
    "# 1. Reshape F to F^{blk} -> [B, B', T/B, C]\n",
    "B_prime = steps // block_size  # 블록 개수\n",
    "F_blk = x.reshape(batch_size, B_prime, block_size, num_features)\n",
    "print(\"F_blk shape (Reshaped into blocks):\", F_blk.shape)  # [B, B', T/B, C]\n",
    "\n",
    "# 2. Pooling: 각 블록의 데이터를 Pooling하여 [B, B', C] 형태로 변환\n",
    "F_pool = F_blk.mean(dim=2)  # Temporal dimension (block size) 평균\n",
    "print(\"F_pool shape (Pooled):\", F_pool.shape)  # [B, B', C]\n",
    "\n",
    "# 3. RNN 적용: 각 블록의 pooled feature를 사용해 γ와 β 생성\n",
    "rnn = nn.LSTM(input_size=num_features, hidden_size=num_features*2, batch_first=True)\n",
    "F_rnn, _ = rnn(F_pool)  # [B, B', C] -> [B, B', 2*C]\n",
    "gamma, beta = torch.chunk(F_rnn, 2, dim=-1)  # γ와 β를 생성\n",
    "print(\"Gamma shape:\", gamma.shape)  # [B, B', C]\n",
    "print(\"Beta shape:\", beta.shape)    # [B, B', C]\n",
    "\n",
    "# 4. Normalization: γ와 β를 적용\n",
    "# F_blk: [B, B', T/B, C], gamma/beta: [B, B', C]\n",
    "gamma = gamma.unsqueeze(2)  # [B, B', 1, C]\n",
    "beta = beta.unsqueeze(2)    # [B, B', 1, C]\n",
    "F_norm = gamma * F_blk + beta\n",
    "print(\"F_norm shape (Normalized):\", F_norm.shape)  # [B, B', T/B, C]\n",
    "\n",
    "# 5. Reshape F_norm back to original shape [B, T, C]\n",
    "F_out = F_norm.reshape(batch_size, steps, num_features)\n",
    "print(\"Output F_out shape:\", F_out.shape)  # [B, T, C]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
